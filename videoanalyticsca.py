# -*- coding: utf-8 -*-
"""VideoAnalyticsCA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13J8lM-cjUCbxaeQXmc7ihh57YvcKrZ_L
"""

import tensorflow as tf
print(tf.test.gpu_device_name())
!/opt/bin/nvidia-smi

import cv2
import os
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from keras.utils import np_utils
from tensorflow.keras.layers import Input,BatchNormalization,Conv3D,MaxPooling3D,AveragePooling3D,Dropout,Flatten,Activation,add,Dense,Average,AveragePooling3D,Concatenate,Conv1D
from tensorflow.keras.losses import categorical_crossentropy
from tensorflow.keras.models import  load_model,Model
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import confusion_matrix
from tensorflow.keras.regularizers import l2
from tensorflow.keras import optimizers
import random
from tensorflow.keras.callbacks import ModelCheckpoint,CSVLogger,EarlyStopping,ReduceLROnPlateau

from google.colab import drive
drive.mount('/content/drive')

os.chdir('/content/drive/My Drive/Colab Notebooks')

label_dic = {
        'd':0,
        'drink':0,
        'e':1,
        'eat':1,
        'groomback':1,
        'gb':1,
        'g':1,
        'groom':1,
        'hang':2,
        'ha':2,
        'head':3,
        'he':3,
        'r':4,
        'rear':4,
        'rest':5,
        'rs':6,
        'w':7,
        'walk':7
        }


classes= ['drink','eat','groom','hang','micromovement','rear','rest','walk']
num_classes= len(classes)

data_path = '/content/drive/My Drive/Colab Notebooks/clipped_database_1/clipped_database/'


video_path_train={}
video_class_train={}
video_path_test = {}
video_class_test={}

index = 0
for i in os.listdir(data_path):
    #print(i)
    if os.path.isdir(data_path+i) and i!='20080420124815':
        for j in os.listdir(data_path+i):
            #print(j)
            file_name=j
            video_path_train[index]=data_path+i+'/'+j
            file_name=file_name.split('_')
            video_class_train[index]=label_dic[file_name[1].strip().lower()]
            index = index+1

for i in os.listdir(data_path):
    if os.path.isdir(data_path+i) and i=='20080420124815':
        for j in os.listdir(data_path+i):
            file_name=j
            video_path_test[index]=data_path+i+'/'+j
            file_name=file_name.split('_')
            video_class_test[index]=label_dic[file_name[1].strip().lower()]
            index = index+1

print(f"Total number of sample video segments to train: {len(video_path_train)}")
print(f"Total number of sample video segments to test: {len(video_path_test)}")

momentum=0.9
epsilon = 0.0005
learning_rate = 0.0001
ra = 0.1
img_width=190
img_height=120
img_depth = 16

class DataGenerator:
  def __init__(self,width,height,depth,data_list,batch_size=32,nchannels=1):
    self.width = width
    self.height = height
    self.depth = depth
    self.batch_size=batch_size
    self.data_list=data_list
    self.cur_train_index=0
    self.channels = nchannels
  def get_data(self,filename,skip=True):
    #print(filename)
    cap=cv2.VideoCapture(filename)
    nframe=cap.get(cv2.CAP_PROP_FRAME_COUNT)
    #print(nframe)
    bAppend = False 
    if(nframe>self.depth):
      if skip:
         #Random skip frames to select 
         s = np.random.randint(int(nframe/self.depth))+1
         #print(s)
         frames=[x*s for x in range(self.depth)]
      else:
        frames = [x for x in range(self.depth)]
    else:
      #print("Insufficient %d frames in video %s, appending the frames" %(nframe,filename))
      bAppend = True
      frames = [x for x in range(int(nframe))] 
    framearray = []
    opticarray=[]
    
    if not bAppend:
      for i in range(len(frames)):
        #print(i,frames[i])
        cap.set(cv2.CAP_PROP_POS_FRAMES,frames[i])
        ret,frame=cap.read()
        #print(frame.shape)
        if ret:
          frame = cv2.resize(frame,(self.width,self.height))
          framearray.append(cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY))      
    else:
      frame=''
      while True:
        ret,frame=cap.read()
        #print(len(framearray))
        if ret:
          frame = cv2.resize(frame,(self.width,self.height))
          framearray.append(cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY))
        else:
          break
      if len(framearray)==0:
        print(filename,nframe)    
      frame=framearray[-1]
      #print(frame.shape)
    cap.release()
    if bAppend:
      while len(framearray)<self.depth:
        framearray.append(frame)

    for i in range(1,len(framearray)):
      flow=cv2.calcOpticalFlowFarneback(framearray[i-1],framearray[i],flow=None,pyr_scale=0.5,levels=5,winsize=11,iterations=10,poly_n=5,poly_sigma=1.1,flags=0)
      flow_x=np.array(flow[...,0])
      flow_y=np.array(flow[...,1])
      flow_temp = flow_x+flow_y
      opticarray.append(flow_temp)
    
    for i in range(len(framearray)):
      framearray[i]=framearray[i]/255
    return np.array(framearray),np.array(opticarray)

  def loaddata_train(self):
    while 1:
      video_frames=[]
      video_optics=[]
      labels=[]
      for i in range(self.batch_size):
        if self.cur_train_index >= len(self.data_list):
          self.cur_train_index=0
        filename = video_path_train[self.data_list[self.cur_train_index]]
        label = video_class_train[self.data_list[self.cur_train_index]]
        video,optic =  self.get_data(filename)
        video_frames.append(video)
        video_optics.append(optic)
        labels.append(np_utils.to_categorical(label,num_classes))
        self.cur_train_index+=1
      video_frames = np.array(video_frames).transpose((0,2,3,1))
      optic_frames = np.array(video_optics).transpose((0,2,3,1))
      labels = np.array(labels)
      video_frames = video_frames.reshape((video_frames.shape[0],self.width,self.height,self.depth,self.channels))       
      optic_frames = optic_frames.reshape((optic_frames.shape[0],self.width,self.height,self.depth-1,self.channels))
      yield ({'Input1':video_frames,'Input2':optic_frames},labels)    
    
  def loaddata_test(self):
    while 1:
      video_frames=[]
      video_optics=[]
      labels=[]
      for i in range(self.batch_size):
        if self.cur_train_index >= len(self.data_list):
          self.cur_train_index=0
        filename = video_path_test[self.data_list[self.cur_train_index]]
        label = video_class_test[self.data_list[self.cur_train_index]]
        video,optic =  self.get_data(filename)
        video_frames.append(video)
        video_optics.append(optic)
        labels.append(np_utils.to_categorical(label,num_classes))
        self.cur_train_index+=1
      video_frames = np.array(video_frames).transpose((0,2,3,1))
      optic_frames = np.array(video_optics).transpose((0,2,3,1))
      labels = np.array(labels)
      video_frames = video_frames.reshape((video_frames.shape[0],self.width,self.height,self.depth,self.channels))       
      optic_frames = optic_frames.reshape((optic_frames.shape[0],self.width,self.height,self.depth-1,self.channels))  
      yield ({'Input1':video_frames,'Input2':optic_frames},labels)
  #Data generator for prediction
  #data contains the list of file name
  def loaddata(self,data):
    while 1:
      video_frames=[]
      video_optics=[]
      for i in range(self.batch_size):
        if self.cur_train_index>=len(self.data_list):
          self.cur_train_index=0
        filename=data[self.data_list[self.cur_train_index]]
        video,optic =  self.get_data(filename)
        video_frames.append(video)
        video_optics.append(optic)
        self.cur_train_index+=1
      video_frames = np.array(video_frames).transpose((0,2,3,1))
      optic_frames = np.array(video_optics).transpose((0,2,3,1))
      video_frames = video_frames.reshape((video_frames.shape[0],self.width,self.height,self.depth,self.channels))       
      optic_frames = optic_frames.reshape((optic_frames.shape[0],self.width,self.height,self.depth-1,self.channels))  
      yield ({'Input1':video_frames,'Input2':optic_frames})

data_list_train = list(video_path_train.keys())
random.shuffle(data_list_train)

data_list_test = list(video_path_test.keys())
random.shuffle(data_list_test)

def simple_residual(Inp,num_filters,kernel_size=3):
  #Temporal space
  x=Conv3D(num_filters,(kernel_size,kernel_size,1),padding='same',use_bias=False)(Inp)
  x = BatchNormalization(axis=-1,momentum=momentum,epsilon=epsilon)(x)
  x = Activation('relu')(x)
  #for temporal analysis
  x = Conv3D(num_filters,kernel_size=(1,1,kernel_size),padding='same',use_bias=False)(x)
  x = BatchNormalization(axis=-1,momentum=momentum,epsilon=epsilon)(x)
  x = Activation('relu')(x)
  return x

def conv2D1D(Inp,num_filters,kernel_size=3):
  x =Conv3D(num_filters,(kernel_size,kernel_size,1),padding='same',use_bias=False)(Inp)
  x = BatchNormalization(axis=-1,momentum=momentum,epsilon=epsilon)(x)
  x = Activation('relu')(x)
  #for temporal analysis
  x = Conv3D(num_filters,kernel_size=(1,1,kernel_size),padding='same',use_bias=False)(x)
  x = BatchNormalization(axis=-1,momentum=momentum,epsilon=epsilon)(x)
  x = Activation('relu')(x)
  return x

def simple_residual_block(Inp,num_filters,kernel_size=3):
  x = Inp
  y = Inp 
  y = simple_residual(y,num_filters,kernel_size)
  x = add([x,y])
  return x

def downsample_residual(Inp,num_filters,kernel_size=3):
  x = Conv3D(num_filters,(kernel_size,kernel_size,1),strides=(2,2,1),padding='same',use_bias=False)(Inp)
  x = BatchNormalization(axis=-1,momentum=momentum,epsilon=epsilon)(x)
  x = Activation('relu')(x)
  x = Conv3D(num_filters,kernel_size=(1,1,kernel_size),strides=(1,1,2),padding='same',use_bias=False)(x)
  x = BatchNormalization(axis=-1,momentum=momentum,epsilon=epsilon)(x)
  x = Activation('relu')(x)
  return x

def simple_downsampleConv3D(Inp,num_filters,kernel_size=3):
  x = Conv3D(num_filters,(kernel_size,kernel_size,kernel_size),padding='same',use_bias=False)(Inp)
  x = BatchNormalization(axis=-1,momentum=momentum,epsilon=epsilon)(x)
  x = Activation('relu')(x)
  return x

def simple_conv3D(Inp,num_filters,kernel_size=3):
  x = Conv3D(num_filters,(kernel_size,kernel_size,kernel_size),padding='same',use_bias=False)(Inp)
  x = BatchNormalization(axis=-1,momentum=momentum,epsilon=epsilon)(x)
  x = Activation('relu')(x)
  return x

def downsample_residualBlk(Inp,num_filters,kernel_size=3):
  x = Inp
  y = Inp
  y = downsample_residual(y,num_filters,kernel_size=kernel_size)
  x = downsample_residual(x,num_filters,kernel_size=1)
  x = add([x,y])
  return x

def feature_extractor_module_video():
  Inp = Input(shape=(img_width,img_height,img_depth,1),name='Input')
  x = conv2D1D(Inp,32)
  #x = simple_conv3D(Inp,32)
  for _ in range(2):
    x = simple_residual_block(x,32)
  #down factor by 2: 8
  x = downsample_residualBlk(x,32)
  #x = simple_conv3D(Inp,64)
  x = conv2D1D(x,64)
  for _ in range(2):
    x = simple_residual_block(x,64)
  x = downsample_residualBlk(x,64)
  #x =  simple_conv3D(x,128)
  x = conv2D1D(x,128)
  for _ in range(4):
    x = simple_residual_block(x,128)
  #down factor by 2: 4  
  x = downsample_residualBlk(x,128)
  #x =  simple_conv3D(x,256)
  x = conv2D1D(x,256)
  for _ in range(8):
    x = simple_residual_block(x,256)
  #down factor by 2: 4    
  x = downsample_residualBlk(x,256)
  #x =  simple_conv3D(x,512)
  x = conv2D1D(x,512)
  for _ in range(8):
    x = simple_residual_block(x,512)
  x = downsample_residualBlk(x,512)
  #x =  simple_conv3D(x,1024)
  x = conv2D1D(x,1024)
  for _ in range(4):
    x = simple_residual_block(x,1024)
  x = conv2D1D(x,2048)
  #x = AveragePooling3D(pool_size=(3,3,3))(x)
  x = Dropout(0.2)(x)
  x = Flatten(name='flatten_feature')(x)
  #x = AveragePooling1D(pool_size=3)(x)
  model = Model(inputs = Inp,outputs=x)
  return model

def feature_extractor_module_optic():
  Inp = Input(shape=(img_width,img_height,img_depth,1),name='Input')
  x = conv2D1D(Inp,32)
  #x = simple_conv3D(Inp,32)
  for _ in range(2):
    x = simple_residual_block(x,32)
  #down factor by 2: 8
  x = downsample_residualBlk(x,32)
  #x = simple_conv3D(Inp,64)
  x = conv2D1D(x,64)
  for _ in range(2):
    x = simple_residual_block(x,64)
  x = downsample_residualBlk(x,64)
  #x =  simple_conv3D(x,128)
  x = conv2D1D(x,128)
  for _ in range(4):
    x = simple_residual_block(x,128)
  #down factor by 2: 4  
  x = downsample_residualBlk(x,128)
  #x =  simple_conv3D(x,256)
  x = conv2D1D(x,256)
  for _ in range(8):
    x = simple_residual_block(x,256)
  #down factor by 2: 4    
  x = downsample_residualBlk(x,256)
  #x =  simple_conv3D(x,512)
  x = conv2D1D(x,512)
  for _ in range(8):
    x = simple_residual_block(x,512)
  x = downsample_residualBlk(x,512)
  #x =  simple_conv3D(x,1024)
  x = conv2D1D(x,1024)
  for _ in range(4):
    x = simple_residual_block(x,1024)
  x = conv2D1D(x,2048)
  #x = AveragePooling3D(pool_size=(3,3,3))(x)
  x = Dropout(0.2)(x)
  x = Flatten(name='flatten_feature')(x)
  #x = AveragePooling1D(pool_size=3)(x)
  model = Model(inputs = Inp,outputs=x)
  return model

feature_motion = feature_extractor_module_video()
feature_optic = feature_extractor_module_optic()
feature_motion.summary()

video = Input(shape=(img_width,img_height,img_depth,1),dtype='float32',name = 'Input1')
optic = Input(shape=(img_width,img_height,img_depth-1,1),dtype='float32',name='Input2')

x1 = feature_motion(video)
x2 = feature_optic(optic)

out = Average()([x1,x2])
#out = Conv1D(32,1,1)(out)
out = Dense(256,activation='relu')(out)
out = Dense(128,activation='relu')(out)

out = Dense(num_classes,activation='softmax')(out)

#out = Dense(num_classes,activation='softmax')(out)
model = Model(inputs=[video,optic],outputs=out)
optz = optimizers.Adam(learning_rate=0.0001)

model_name='video_analytics_new_2'
#log_dir='/content/drive/My Drive/Colab Notebooks/'
filepath='/content/drive/My Drive/Colab Notebook/'+model_name+'.h5py'
filepath2 = '/content/drive/My Drive/Colab Notebook/video_analytics.h5py'
checkpoint = ModelCheckpoint(filepath,monitor='val_acc',save_weights_only=True,save_best_only=True)
csv_logger=CSVLogger("/content/drive/My Drive/Colab NoteBooks"+model_name+".csv")
reduce_lr = ReduceLROnPlateau(monitor='val_loss',factor=0.1,paitence=3,verbose=1)
early_stopping = EarlyStopping(monitor='val_loss',min_delta=0,patience=40,verbose=1)
model.compile(loss='categorical_crossentropy',optimizer=optz,metrics=['accuracy'])
model.summary()

batch_size=2

gen_train = DataGenerator(img_width,img_height,img_depth,data_list_train,batch_size)
gen_test = DataGenerator(img_width,img_height,img_depth,data_list_test,batch_size)
model.load_weights(filepath)

model.fit_generator(gen_train.loaddata_train(),steps_per_epoch = max(1,len(data_list_train)//batch_size),validation_data=gen_test.loaddata_test(),validation_steps=max(1,len(data_list_test)//batch_size),validation_freq=2,epochs=2,callbacks=[checkpoint,csv_logger,reduce_lr,early_stopping])

import matplotlib.pyplot as plt

dict1_lab = video_class_train
dict1 = video_path_train

dict2_lab = video_class_test
dict2 = video_path_test

dict1.update(dict2)
dict1_lab.update(dict2_lab)

y = np.array(list(dict1_lab.values()))

data_gen = DataGenerator(img_width,img_height,img_depth,list(dict1.keys()),batch_size=1)

y_pred=model.predict_generator(data_gen.loaddata(dict1),steps=len(dict1))

y_pred

y_test = np.argmax(y_pred,axis=-1)

y_test

import sklearn.metrics as metrics

metrics.confusion_matrix(y,y_test)

metrics.accuracy_score(y,y_test)*100

def plot_confusion_matrix(y_true,y_pred,classes,normalize=False,title=None,cmap=plt.cm.Oranges):
  if not title:
    if not normalize:
      title='Confusion Matrix'
    else:
      title='Normalized Confusion matrix'
  cm=confusion_matrix(y_true,y_pred)
  fig,ax = plt.subplots()
  im = ax.imshow(cm,interpolation='nearest',cmap=cmap)
  ax.figure.colorbar(im,ax=ax)
  ax.set(xticks=np.arange(cm.shape[1]),yticks=np.arange(cm.shape[0]),xticklabels=classes,yticklabels=classes,title=title,ylabel='True label',xlabel='Predicted label')
  plt.setp(ax.get_xticklabels(),rotation=45,ha="right",rotation_mode="anchor")
  fmt = ".2f" if normalize else 'd'
  thresh = cm.max()/2.
  for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
      ax.text(j,i,format(cm[i,j],fmt),ha="center",va="center",color="white" if cm[i,j]> thresh else "black")
  fig.tight_layout()
  return ax

plot_confusion_matrix(y,y_test,classes)



def prepare_data(list_frames):
    #print(filename)
    nframe=len(list_frames)
    #print(nframe)
    bAppend = False 
    if(nframe>16):
      s = np.random.randint(int(nframe/16))+1
      frames=[x*s for x in range(16)]
    else:
      #print("Insufficient %d frames in video %s, appending the frames" %(nframe,filename))
      bAppend = True
      frames = [x for x in range(int(nframe))] 
    framearray = []
    opticarray=[]
    
    if not bAppend:
      #print(i,frames[i])
      for j in frames:
        f = list_frame[j]
        f = cv2.resize(f,(120,190))
        framearray.append(cv2.cvtColor(f,cv2.COLOR_BGR2GRAY))
    
    else:
      frame=''
      for j in frames:
        f = list_frame[j]
        f = cv2.resize(f,(120,190))
        framearray.append(cv2.cvtColor(f,cv2.COLOR_BGR2GRAY))
      frame=framearray[-1]  
      while len(framearray)<16:
        framearray.append(frame)

    for i in range(1,len(framearray)):
      flow=cv2.calcOpticalFlowFarneback(framearray[i-1],framearray[i],flow=None,pyr_scale=0.5,levels=5,winsize=11,iterations=10,poly_n=5,poly_sigma=1.1,flags=0)
      flow_x=np.array(flow[...,0])
      flow_y=np.array(flow[...,1])
      flow_temp = flow_x+flow_y
      opticarray.append(flow_temp)
    
    for i in range(len(framearray)):
      framearray[i]=framearray[i]/255
    framearray = np.array(framearray)
    opticarray = np.array(opticarray)
    framearray = np.expand_dims(framearray,0)
    opticarray = np.expand_dims(opticarray,0)
    framearray = framearray.transpose((0,2,3,1))
    opticarray = opticarray.transpose((0,2,3,1))
    framearray = np.expand_dims(framearray,-1)  
    opticarray = np.expand_dims(opticarray,-1)
    return np.array(framearray),np.array(opticarray)

def make_prediction(list_frame):
  frame,opticarray = prepare_data(list_frame)
  y = model.predict({'Input1':frame,'Input2':opticarray})
  y = np.argmax(y,axis=-1)
  return classes[int(y)]

cap = cv2.VideoCapture('/content/drive/My Drive/Train_Dataset/prediction_new.avi')
fourcc = cv2.VideoWriter_fourcc(*'XVID')
out = cv2.VideoWriter('/content/drive/My Drive/Train_Dataset/predicted_new.avi',fourcc,30.0,(360,240))
list_frame=[]
font = cv2.FONT_HERSHEY_SIMPLEX

index=0
start_index=0
while cap.isOpened():
  ret,frame = cap.read()
  if ret:
    if len(list_frame)==16:
      prediction = make_prediction(list_frame)
      for i in range(8):
        f = list_frame.pop(0)
        cv2.putText(f,prediction,(30,30),font,1,(255,255,255),2,cv2.LINE_AA)
        out.write(cv2.resize(f,(360,240)))
      list_frame.append(frame)
    else:
      list_frame.append(frame)
    index+=1
    print(index)  
  else:
    break

cap.release()
out.release()

